%===================================================================================
% Chapter: Results
%===================================================================================
\chapter{Resultados}\label{chapter:results}
% \addcontentsline{toc}{chapter}{Análisis del conjunto de datos}
%===================================================================================

% introducción
Para iniciar la experimentación fue necesario normalizar los datos y luego se realizaron tres fases de experimentos. En la primera fase se probaron varias arquitecturas de red con el conjunto de datos original. Para la segunda fase se analizó la reducción de dimensiones utilizando el algoritmo de \'arboles aleatorios. Como ultima fase se modificó el conjunto de datos original para obtener uno nuevo y probar los algoritmos anteriores.

\section{Procesamiento de los datos}
% proceso de limpieza de datos
El conjunto de datos posee 42 características por cada entrada, las primeras 41 forman parte de la descripción del paquete de red y la última es la clasificación de este. Esta ultima puede variar, en los archivos con la extensión arff la clase es normal o anómalo, por otro lado, en los archivos txt, si el paquete es de un ataque, se especifica el tipo de ataque. Como se mencionó anteriormente, este documento se centra en la clasificación de los datos en normal y anómalos.

Para el proceso de normalización se investigaron todos los datos en los conjuntos de entrenamiento y de prueba. Las características de cada uno pueden ser de dos tipos, numéricas con valores reales o cualitativas con valores predefinidos. En el caso de las variables numéricas se escalaron todos sus valores al rango [0, 1] buscando el valor mínimo y máximo:
\[A_{i} = \frac{A_{i} - Min_{i}}{Max_{i} - Min_{i}}\] 
donde $i$ es la $i$-\'esima característica, $A_{i}$ es el vector de los valores de la característica de cada dato y $Min_{i}$, $Max_{i}$ son los valores mínimo y máximo de la característica respectivamente. Por otra parte, en las variables cualitativas se le asignó un índice a cada uno de los posibles valores comenzando por el 0, luego para escalarlos se dividió cada uno por la cantidad de posibles valores menos 1 para así obtener los datos en el rango [0, 1]:
\[A_{i} = \frac{index(A_{i})}{len(i) - 1}\]

\section{Modelos básicos}
% arquitecturas de redes simples con el dataset original
% 4x50                  CT-75.1  21T-52.76
% 64-128-64-32-16       CT-75.4  21T-53.25
% 64-128-128-64-32-16   CT-77.91 21T-58
Para un primer acercamiento al problema y para trazar una línea base superior al 50\% del algoritmo aleatorio se construyeron redes neuronales que entrenaron y se evaluaron con el conjunto de datos original. La primera arquitectura a probar posee 4 capaz ocultas y cada una con 50 neuronas:
\begin{verbatim}
    from keras import models, layers

    model = models.Sequential()

    # Input - Layer
    model.add(layers.Dense(50, input_dim=(41), 
        activation='relu', name='input_layer'))

    # Hidden - Layers
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(50, activation='relu', 
        name='hidden_layer_1'))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(50, activation='relu', 
        name='hidden_layer_2'))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(50, activation='relu', 
        name='hidden_layer_3'))

    # Output - Layer
    model.add(layers.Dense(1, activation='sigmoid', 
        name='output_layer'))
\end{verbatim}

En esta arquitectura, como en las restantes, se utilizó como función de activación en las capaz intermedias $relu$:
\[f(x)=max(0,x)\]
esta función es muy eficiente por si sencillez por lo que agiliza el proceso de entrenamiento. Por otra parte, en la capa de salida es necesaria la función $sigmoid$
\[f(x) = \frac{1}{1 + e^{-x}}\]
que tiene como resultado valores entre 0 y 1, dado que los datos de entrenamiento fueron modificados y los casos de tráfico anómalo tienen valor 1 y los normales valor 0 en su campo de clasificación, tomando los casos $< 0.5$ como tráfico normal y los $\geq 0.5$ tráfico anómalo

Este trabajo se centra en explorar los resultados de utilizar redes neuronales simples en el proceso de detección de intrusos, por lo que se analiza el comportamiento de los paquetes que viajan en la red por separado. Por lo antes mencionado, todos los modelos a probar utilizan capaz completamente conectadas o capaz densas como también se les conocen. La salida de estas capaz está representada por la siguiente función:
\[output = activation(dot(input, kernel) + bias)\]
donde $activation$ es la función de activación escogida, $dot$ es el producto punto entre dos vectores, $input$ es el vector con los valores de entrada, $kernel$ son los valores de peso de la red y $bias$ representa un valor de sesgo utilizado en los algoritmos de aprendizaje de máquina para optimizar los modelos.

% TODO: poner gráficas de los resultados de estos primeros modelos y explicar la función de optimizacion, la de perdida y las métricas
% Primeros resultados con diferentes arquitecturas de redes neuronales, modificando la cantidad de capas y el numero de neuronas en alguna de estas

\section{Selección de características}
% selección de features
Explicación y muestreo de resultados de la aplicación del algoritmo Random Forest para la extracción de las características de mayor importancia para la clasificación

% 30 features           CT-79.87 21T-61.85  76.79  55.90
% 64 batch size         CT-78.33 21T-58.82  77.07  56.43  77.13  56.53

\section{Transformación del conjunto de datos}
% transformación del conjunto de datos
Unificación de los conjuntos de datos disjuntos del dataset original para la posterior creación de uno nuevo el cual contenga elementos de todas las dificultades

\subsection{Modificación del valor de aprendizaje}
Variación del learning rate en el proceso de aprendizaje y búsqueda para el mejor resultado

\subsection{Validación por partes}
Explicación del algoritmo de K-fold para una aproximación a un resultado mas real

% k-fold validation
% búsqueda del learning rate

% TODO: demostrar o probar porque con el dataset transformado no es necesario volver a probar los primeros modelos que eran mas malos con el dataset original